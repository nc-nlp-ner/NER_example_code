{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRF based NER\n",
    "- [참고 코드](https://lovit.github.io/nlp/2018/06/22/crf_based_ner/)\n",
    "- data : CoNLL 2002 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-crfsuite\n",
      "  Downloading python_crfsuite-0.9.7-cp37-cp37m-macosx_10_13_x86_64.whl (186 kB)\n",
      "\u001b[K     |████████████████████████████████| 186 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: python-crfsuite\n",
      "Successfully installed python-crfsuite-0.9.7\n"
     ]
    }
   ],
   "source": [
    "!pip install python-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pycrfsuite\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from itertools import chain\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to /Users/mac/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2002.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['esp.testa', 'esp.testb', 'esp.train', 'ned.testa', 'ned.testb', 'ned.train']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다운로드 및 field확인\n",
    "nltk.download('conll2002')\n",
    "nltk.corpus.conll2002.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nltk.corpus.conll2002\n",
    "train_sents =  list(data.iob_sents('esp.train'))\n",
    "test_sents = list(data.iob_sents('esp.testb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Melbourne', 'NP', 'B-LOC'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('Australia', 'NP', 'B-LOC'),\n",
       " (')', 'Fpt', 'O'),\n",
       " (',', 'Fc', 'O'),\n",
       " ('25', 'Z', 'O'),\n",
       " ('may', 'NC', 'O'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('EFE', 'NC', 'B-ORG'),\n",
       " (')', 'Fpt', 'O'),\n",
       " ('.', 'Fp', 'O')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column : token | pos_tag | ner_tag\n",
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. using PyCRFSuite \n",
    "## PyCRFSuite란?\n",
    "- c++로 구현된 CRFSuite 구현체를 Python 환경에서 이용할 수 있도록 도와주는 라이브러리\n",
    "- 해당 라이브러리를 이용하기위해서는 potential function을 직접 디자인해야 함\n",
    "\n",
    "### potential function\n",
    "- word2features는 문장 sent의 시점 i에 대한 potential function임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(), # word lower\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(), # word is upper?\n",
    "        'word.istitle=%s' % word.istitle(), # word is title?\n",
    "        'word.isdigit=%s' % word.isdigit(), # word is digit?\n",
    "        'postag=' + postag,\n",
    "        'postag[:2]=' + postag[:2],\n",
    "    ]\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:postag=' + postag1,\n",
    "            '-1:postag[:2]=' + postag1[:2],\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "    if i < len(sent) - 1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:postag=' + postag1,\n",
    "            '+1:postag[:2]=' + postag1[:2],\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "                \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence to features, labels, tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for  i in range(len(sent))]\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bias',\n",
       " 'word.lower=melbourne',\n",
       " 'word[-3:]=rne',\n",
       " 'word[-2:]=ne',\n",
       " 'word.isupper=False',\n",
       " 'word.istitle=True',\n",
       " 'word.isdigit=False',\n",
       " 'postag=NP',\n",
       " 'postag[:2]=NP',\n",
       " 'BOS',\n",
       " '+1:word.lower=(',\n",
       " '+1:word.istitle=False',\n",
       " '+1:word.isupper=False',\n",
       " '+1:postag=Fpa',\n",
       " '+1:postag[:2]=Fp']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(train_sents[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 학습 가능한 형태의 데이터로 변환\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the model\n",
    "## 3-1. 주어진 모든 feature를 다 가지고 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 데이터를 append 하여 학습할 준비 함\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq,yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter setting\n",
    "# 최소 다섯번 이상 등장한 feature만 이용\n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True,\n",
    "    \n",
    "    # minimum frequency\n",
    "    'feature.minfreq': 5\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "data_name = 'conll2002-esp.crfsuite'\n",
    "trainer.train(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7fc3097c1d50>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger() # 학습된 모델을 tagger로 불러옴\n",
    "tagger.open(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La Coruña , 23 may ( EFECOM ) .\n",
      "\n",
      "Predicted: B-LOC, I-LOC, O, O, O, O, B-ORG, O, O\n",
      "Correct: B-LOC, I-LOC, O, O, O, O, B-ORG, O, O\n"
     ]
    }
   ],
   "source": [
    "# 테스트 문장에 대하여 ner tagging 수행\n",
    "ex_sent = test_sents[0]\n",
    "print(' '.join(sent2tokens(ex_sent)), end='\\n\\n')\n",
    "print(\"Predicted:\",', '.join(tagger.tag(sent2features(ex_sent))))\n",
    "print(\"Correct:\",', '.join(sent2labels(ex_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tagging performance\n",
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = []\n",
    "for sent in test_sents:\n",
    "    y_pred.append(tagger.tag(sent2features(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n       B-LOC       0.74      0.71      0.73      1084\\n       I-LOC       0.57      0.51      0.54       325\\n      B-MISC       0.61      0.37      0.46       339\\n      I-MISC       0.59      0.43      0.50       557\\n       B-ORG       0.76      0.78      0.77      1400\\n       I-ORG       0.78      0.76      0.77      1104\\n       B-PER       0.77      0.87      0.82       735\\n       I-PER       0.83      0.94      0.88       634\\n\\n   micro avg       0.75      0.72      0.73      6178\\n   macro avg       0.71      0.67      0.68      6178\\nweighted avg       0.74      0.72      0.73      6178\\n samples avg       0.09      0.09      0.09      6178\\n'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio_classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. 한정된 feature만 가지고 학습\n",
    "- bias, word lower, word[-3:], word[-2:]만 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "    ]\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "                \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 학습 가능한 형태의 데이터로 변환\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 데이터를 append 하여 학습할 준비 함\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq,yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter setting\n",
    "# 최소 다섯번 이상 등장한 feature만 이용\n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True,\n",
    "    \n",
    "    # minimum frequency\n",
    "    'feature.minfreq': 5\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7fc34e564390>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "data_name = 'conll2002-esp.crfsuite'\n",
    "trainer.train(data_name)\n",
    "tagger = pycrfsuite.Tagger() # 학습된 모델을 tagger로 불러옴\n",
    "tagger.open(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = []\n",
    "for sent in test_sents:\n",
    "    y_pred.append(tagger.tag(sent2features(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n       B-LOC       0.69      0.49      0.58      1084\\n       I-LOC       0.60      0.47      0.52       325\\n      B-MISC       0.52      0.20      0.29       339\\n      I-MISC       0.52      0.36      0.43       557\\n       B-ORG       0.74      0.55      0.63      1400\\n       I-ORG       0.71      0.52      0.60      1104\\n       B-PER       0.83      0.69      0.76       735\\n       I-PER       0.86      0.86      0.86       634\\n\\n   micro avg       0.72      0.54      0.62      6178\\n   macro avg       0.68      0.52      0.58      6178\\nweighted avg       0.71      0.54      0.61      6178\\n samples avg       0.07      0.07      0.07      6178\\n'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio_classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.모델 확인\n",
    "- 영향력이 높은 features, 각각에 해당하는 weight확인\n",
    "- 3-1에서 모든 feature를 이용했던 모델로 평가\n",
    "- 해당 결과로 ner tagging에서 중요한 정보는 앞/뒤에 등장하는 단어임을 알 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('-1:word.lower=nuboso', 'B-LOC') : 4.041605\n",
      "('word.lower=líbano', 'B-LOC') : 3.074029\n",
      "('-1:word.lower=calle', 'I-LOC') : 2.933811\n",
      "('word.lower=vitoria', 'B-LOC') : 2.886962\n",
      "('-1:word.lower=despejado', 'B-LOC') : 2.829584\n",
      "('-1:word.lower=cantabria', 'B-LOC') : 2.817601\n",
      "('word.lower=pamplona', 'B-LOC') : 2.659921\n",
      "('-1:word.lower=santa', 'I-LOC') : 2.575717\n",
      "('-1:word.lower=desde', 'B-LOC') : 2.571576\n",
      "('word.lower=londres', 'B-LOC') : 2.439067\n",
      "('word.lower=melilla', 'B-LOC') : 2.377884\n",
      "('word.lower=palacio', 'B-LOC') : 2.289678\n",
      "('word.lower=cáceres', 'B-LOC') : 2.284109\n",
      "('word.lower=bruselas', 'B-LOC') : 2.252099\n",
      "('+1:word.lower=coruña', 'B-LOC') : 2.143314\n",
      "('word.lower=cantabria', 'B-LOC') : 2.088536\n",
      "('word.lower=badajoz', 'B-LOC') : 2.074211\n",
      "('word.lower=santander', 'B-LOC') : 2.048165\n",
      "('-1:word.lower=plaza', 'I-LOC') : 2.024622\n",
      "('-1:word.lower=en', 'B-LOC') : 1.979929\n",
      "('-1:word.lower=de', 'I-LOC') : 1.978107\n",
      "('word.lower=estadio', 'B-LOC') : 1.977449\n",
      "('-1:word.lower=ciudad', 'I-LOC') : 1.969862\n",
      "('word.lower=ceuta', 'B-LOC') : 1.945646\n",
      "('word.lower=israel', 'B-LOC') : 1.942993\n",
      "('-1:word.lower=estadio', 'I-LOC') : 1.929035\n",
      "('word.lower=murcia', 'B-LOC') : 1.876291\n",
      "('-1:word.lower=hacia', 'B-LOC') : 1.853855\n",
      "('word.lower=rfa', 'B-LOC') : 1.795125\n",
      "('word[-3:]=RFA', 'B-LOC') : 1.795125\n",
      "('word[-2:]=ua', 'B-LOC') : 1.766392\n",
      "('word[-3:]=jón', 'B-LOC') : 1.744715\n",
      "('word.lower=washington', 'B-LOC') : 1.716246\n",
      "('-1:word.lower=san', 'I-LOC') : 1.71113\n",
      "('word.lower=roma', 'B-LOC') : 1.708192\n",
      "('word.lower=plasencia', 'B-LOC') : 1.706833\n",
      "('word.lower=fráncfort', 'B-LOC') : 1.704078\n",
      "('word.lower=coruña', 'I-LOC') : 1.675446\n",
      "('word[-3:]=joz', 'B-LOC') : 1.644774\n",
      "('word.lower=eritrea', 'B-LOC') : 1.642121\n",
      "('word[-2:]=UU', 'B-LOC') : 1.641769\n",
      "('word[-3:]=bia', 'B-LOC') : 1.638622\n",
      "('word[-3:]=let', 'B-LOC') : 1.636044\n",
      "('word.lower=marruecos', 'B-LOC') : 1.614872\n",
      "('word.lower=venezuela', 'B-LOC') : 1.612351\n",
      "('word.lower=gijón', 'B-LOC') : 1.60959\n",
      "('word.istitle=True', 'B-LOC') : 1.601276\n",
      "('word.lower=galicia', 'B-LOC') : 1.575935\n",
      "('-1:word.isupper=True', 'B-LOC') : 1.552731\n",
      "('-1:word.lower=palacio', 'I-LOC') : 1.542466\n"
     ]
    }
   ],
   "source": [
    "debugger = tagger.info()\n",
    "weights = debugger.state_features\n",
    "location_features = {feature:weight for feature, weight in weights.items() if 'LOC' in feature[1]}\n",
    "\n",
    "for feature, weight in sorted(location_features.items(), key=lambda x:-x[1])[:50]:\n",
    "    print('{} : {}'.format(feature, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
